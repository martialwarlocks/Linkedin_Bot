{"ast":null,"code":"// Custom Document Management System\n// Mimics SpikedAI functionality but runs locally\n\nclass DocumentManager {\n  constructor() {\n    this.documents = JSON.parse(localStorage.getItem('customDocuments') || '[]');\n    this.chunks = JSON.parse(localStorage.getItem('documentChunks') || '[]');\n    this.embeddings = JSON.parse(localStorage.getItem('documentEmbeddings') || '[]');\n  }\n\n  // Save data to localStorage\n  saveData() {\n    localStorage.setItem('customDocuments', JSON.stringify(this.documents));\n    localStorage.setItem('documentChunks', JSON.stringify(this.chunks));\n    localStorage.setItem('documentEmbeddings', JSON.stringify(this.embeddings));\n  }\n\n  // Extract text from different file types\n  async extractTextFromFile(file) {\n    return new Promise((resolve, reject) => {\n      const reader = new FileReader();\n      reader.onload = async e => {\n        try {\n          const content = e.target.result;\n          let text = '';\n          if (file.type === 'text/plain') {\n            text = content;\n          } else if (file.type === 'application/pdf') {\n            // For PDF, we'll use a simple text extraction\n            // In production, you'd use a proper PDF parser\n            text = this.extractTextFromPDF(content);\n          } else if (file.type.includes('word') || file.type.includes('document')) {\n            // For Word documents, extract text content\n            text = this.extractTextFromWord(content);\n          } else {\n            // Fallback: try to extract as text\n            text = content.toString();\n          }\n          resolve(text);\n        } catch (error) {\n          reject(error);\n        }\n      };\n      reader.onerror = reject;\n      if (file.type === 'text/plain') {\n        reader.readAsText(file);\n      } else {\n        reader.readAsArrayBuffer(file);\n      }\n    });\n  }\n\n  // Simple PDF text extraction (basic implementation)\n  extractTextFromPDF(arrayBuffer) {\n    // This is a simplified version. In production, use a proper PDF parser\n    const uint8Array = new Uint8Array(arrayBuffer);\n    const text = new TextDecoder().decode(uint8Array);\n\n    // Basic text extraction - remove binary data\n    const textMatch = text.match(/\\(([^)]+)\\)/g);\n    if (textMatch) {\n      return textMatch.join(' ').replace(/[^\\w\\s]/g, ' ');\n    }\n    return text.replace(/[^\\w\\s]/g, ' ').substring(0, 10000); // Limit text length\n  }\n\n  // Simple Word document text extraction\n  extractTextFromWord(arrayBuffer) {\n    const uint8Array = new Uint8Array(arrayBuffer);\n    const text = new TextDecoder().decode(uint8Array);\n\n    // Extract text content from Word document\n    const textMatch = text.match(/<w:t[^>]*>([^<]+)<\\/w:t>/g);\n    if (textMatch) {\n      return textMatch.map(match => match.replace(/<[^>]+>/g, '')).join(' ');\n    }\n    return text.replace(/[^\\w\\s]/g, ' ').substring(0, 10000);\n  }\n\n  // Chunk text into smaller pieces for better processing\n  chunkText(text, chunkSize = 1000, overlap = 200) {\n    const chunks = [];\n    let start = 0;\n    while (start < text.length) {\n      const end = Math.min(start + chunkSize, text.length);\n      const chunk = text.substring(start, end);\n      chunks.push({\n        id: `chunk_${Date.now()}_${start}`,\n        text: chunk,\n        start: start,\n        end: end\n      });\n      start = end - overlap;\n    }\n    return chunks;\n  }\n\n  // Simple text similarity (cosine similarity)\n  calculateSimilarity(text1, text2) {\n    const words1 = text1.toLowerCase().split(/\\s+/);\n    const words2 = text2.toLowerCase().split(/\\s+/);\n    const wordSet = new Set([...words1, ...words2]);\n    const vector1 = Array.from(wordSet).map(word => words1.filter(w => w === word).length);\n    const vector2 = Array.from(wordSet).map(word => words2.filter(w => w === word).length);\n    const dotProduct = vector1.reduce((sum, val, i) => sum + val * vector2[i], 0);\n    const magnitude1 = Math.sqrt(vector1.reduce((sum, val) => sum + val * val, 0));\n    const magnitude2 = Math.sqrt(vector2.reduce((sum, val) => sum + val * val, 0));\n    return dotProduct / (magnitude1 * magnitude2);\n  }\n\n  // Upload and process document\n  async uploadDocument(file) {\n    try {\n      // Extract text from file\n      const text = await this.extractTextFromFile(file);\n\n      // Create document record\n      const document = {\n        id: `doc_${Date.now()}`,\n        filename: file.name,\n        fileType: file.type,\n        fileSize: file.size,\n        uploadDate: new Date().toISOString(),\n        text: text,\n        chunks: []\n      };\n\n      // Chunk the text\n      const chunks = this.chunkText(text);\n      document.chunks = chunks.map(chunk => chunk.id);\n\n      // Add chunks to global chunks array\n      this.chunks.push(...chunks);\n\n      // Add document to documents array\n      this.documents.push(document);\n\n      // Save to localStorage\n      this.saveData();\n      return {\n        success: true,\n        document: document,\n        chunksAdded: chunks.length,\n        totalChunks: this.chunks.length\n      };\n    } catch (error) {\n      console.error('Error uploading document:', error);\n      throw error;\n    }\n  }\n\n  // Search documents for relevant content\n  searchDocuments(query, maxResults = 5) {\n    const queryLower = query.toLowerCase();\n    const results = [];\n\n    // Search through all chunks\n    this.chunks.forEach(chunk => {\n      const similarity = this.calculateSimilarity(queryLower, chunk.text.toLowerCase());\n      if (similarity > 0.1) {\n        // Threshold for relevance\n        results.push({\n          chunk: chunk,\n          similarity: similarity,\n          document: this.documents.find(doc => doc.chunks.includes(chunk.id))\n        });\n      }\n    });\n\n    // Sort by similarity and return top results\n    return results.sort((a, b) => b.similarity - a.similarity).slice(0, maxResults);\n  }\n\n  // Get all documents\n  getDocuments() {\n    return this.documents.map(doc => ({\n      id: doc.id,\n      filename: doc.filename,\n      fileType: doc.fileType,\n      fileSize: doc.fileSize,\n      uploadDate: doc.uploadDate,\n      chunkCount: doc.chunks.length\n    }));\n  }\n\n  // Delete document\n  deleteDocument(documentId) {\n    const documentIndex = this.documents.findIndex(doc => doc.id === documentId);\n    if (documentIndex === -1) {\n      throw new Error('Document not found');\n    }\n    const document = this.documents[documentIndex];\n\n    // Remove chunks associated with this document\n    this.chunks = this.chunks.filter(chunk => !document.chunks.includes(chunk.id));\n\n    // Remove document\n    this.documents.splice(documentIndex, 1);\n\n    // Save to localStorage\n    this.saveData();\n    return {\n      success: true\n    };\n  }\n\n  // Crawl website content (simplified)\n  async crawlWebsite(url) {\n    try {\n      // This is a simplified web crawler\n      // In production, you'd use a proper web scraping library\n      const response = await fetch(`https://api.allorigins.win/get?url=${encodeURIComponent(url)}`);\n      const data = await response.json();\n      if (data.contents) {\n        // Extract text content from HTML\n        const text = this.extractTextFromHTML(data.contents);\n\n        // Create document record\n        const document = {\n          id: `web_${Date.now()}`,\n          filename: `Web Content - ${url}`,\n          fileType: 'text/html',\n          fileSize: text.length,\n          uploadDate: new Date().toISOString(),\n          text: text,\n          url: url,\n          chunks: []\n        };\n\n        // Chunk the text\n        const chunks = this.chunkText(text);\n        document.chunks = chunks.map(chunk => chunk.id);\n\n        // Add chunks to global chunks array\n        this.chunks.push(...chunks);\n\n        // Add document to documents array\n        this.documents.push(document);\n\n        // Save to localStorage\n        this.saveData();\n        return {\n          success: true,\n          document: document,\n          chunksAdded: chunks.length,\n          totalChunks: this.chunks.length\n        };\n      } else {\n        throw new Error('Failed to fetch website content');\n      }\n    } catch (error) {\n      console.error('Error crawling website:', error);\n      throw error;\n    }\n  }\n\n  // Extract text from HTML\n  extractTextFromHTML(html) {\n    // Simple HTML to text conversion\n    const div = document.createElement('div');\n    div.innerHTML = html;\n\n    // Remove script and style elements\n    const scripts = div.querySelectorAll('script, style');\n    scripts.forEach(script => script.remove());\n\n    // Get text content\n    let text = div.textContent || div.innerText || '';\n\n    // Clean up text\n    text = text.replace(/\\s+/g, ' ').trim();\n    return text.substring(0, 50000); // Limit text length\n  }\n\n  // Get document content for AI processing\n  getDocumentContent(documentId) {\n    const document = this.documents.find(doc => doc.id === documentId);\n    if (!document) {\n      throw new Error('Document not found');\n    }\n    return document.text;\n  }\n\n  // Get relevant chunks for a query\n  getRelevantChunks(query, maxChunks = 10) {\n    const searchResults = this.searchDocuments(query, maxChunks);\n    return searchResults.map(result => ({\n      text: result.chunk.text,\n      similarity: result.similarity,\n      source: result.document.filename\n    }));\n  }\n}\nexport default DocumentManager;","map":{"version":3,"names":["DocumentManager","constructor","documents","JSON","parse","localStorage","getItem","chunks","embeddings","saveData","setItem","stringify","extractTextFromFile","file","Promise","resolve","reject","reader","FileReader","onload","e","content","target","result","text","type","extractTextFromPDF","includes","extractTextFromWord","toString","error","onerror","readAsText","readAsArrayBuffer","arrayBuffer","uint8Array","Uint8Array","TextDecoder","decode","textMatch","match","join","replace","substring","map","chunkText","chunkSize","overlap","start","length","end","Math","min","chunk","push","id","Date","now","calculateSimilarity","text1","text2","words1","toLowerCase","split","words2","wordSet","Set","vector1","Array","from","word","filter","w","vector2","dotProduct","reduce","sum","val","i","magnitude1","sqrt","magnitude2","uploadDocument","document","filename","name","fileType","fileSize","size","uploadDate","toISOString","success","chunksAdded","totalChunks","console","searchDocuments","query","maxResults","queryLower","results","forEach","similarity","find","doc","sort","a","b","slice","getDocuments","chunkCount","deleteDocument","documentId","documentIndex","findIndex","Error","splice","crawlWebsite","url","response","fetch","encodeURIComponent","data","json","contents","extractTextFromHTML","html","div","createElement","innerHTML","scripts","querySelectorAll","script","remove","textContent","innerText","trim","getDocumentContent","getRelevantChunks","maxChunks","searchResults","source"],"sources":["/Users/yatins/linkedin-intelligence-system/Linkedin_Bot/linkedin-content-creator/src/documentManager.js"],"sourcesContent":["// Custom Document Management System\n// Mimics SpikedAI functionality but runs locally\n\nclass DocumentManager {\n  constructor() {\n    this.documents = JSON.parse(localStorage.getItem('customDocuments') || '[]');\n    this.chunks = JSON.parse(localStorage.getItem('documentChunks') || '[]');\n    this.embeddings = JSON.parse(localStorage.getItem('documentEmbeddings') || '[]');\n  }\n\n  // Save data to localStorage\n  saveData() {\n    localStorage.setItem('customDocuments', JSON.stringify(this.documents));\n    localStorage.setItem('documentChunks', JSON.stringify(this.chunks));\n    localStorage.setItem('documentEmbeddings', JSON.stringify(this.embeddings));\n  }\n\n  // Extract text from different file types\n  async extractTextFromFile(file) {\n    return new Promise((resolve, reject) => {\n      const reader = new FileReader();\n      \n      reader.onload = async (e) => {\n        try {\n          const content = e.target.result;\n          let text = '';\n\n          if (file.type === 'text/plain') {\n            text = content;\n          } else if (file.type === 'application/pdf') {\n            // For PDF, we'll use a simple text extraction\n            // In production, you'd use a proper PDF parser\n            text = this.extractTextFromPDF(content);\n          } else if (file.type.includes('word') || file.type.includes('document')) {\n            // For Word documents, extract text content\n            text = this.extractTextFromWord(content);\n          } else {\n            // Fallback: try to extract as text\n            text = content.toString();\n          }\n\n          resolve(text);\n        } catch (error) {\n          reject(error);\n        }\n      };\n\n      reader.onerror = reject;\n\n      if (file.type === 'text/plain') {\n        reader.readAsText(file);\n      } else {\n        reader.readAsArrayBuffer(file);\n      }\n    });\n  }\n\n  // Simple PDF text extraction (basic implementation)\n  extractTextFromPDF(arrayBuffer) {\n    // This is a simplified version. In production, use a proper PDF parser\n    const uint8Array = new Uint8Array(arrayBuffer);\n    const text = new TextDecoder().decode(uint8Array);\n    \n    // Basic text extraction - remove binary data\n    const textMatch = text.match(/\\(([^)]+)\\)/g);\n    if (textMatch) {\n      return textMatch.join(' ').replace(/[^\\w\\s]/g, ' ');\n    }\n    \n    return text.replace(/[^\\w\\s]/g, ' ').substring(0, 10000); // Limit text length\n  }\n\n  // Simple Word document text extraction\n  extractTextFromWord(arrayBuffer) {\n    const uint8Array = new Uint8Array(arrayBuffer);\n    const text = new TextDecoder().decode(uint8Array);\n    \n    // Extract text content from Word document\n    const textMatch = text.match(/<w:t[^>]*>([^<]+)<\\/w:t>/g);\n    if (textMatch) {\n      return textMatch.map(match => match.replace(/<[^>]+>/g, '')).join(' ');\n    }\n    \n    return text.replace(/[^\\w\\s]/g, ' ').substring(0, 10000);\n  }\n\n  // Chunk text into smaller pieces for better processing\n  chunkText(text, chunkSize = 1000, overlap = 200) {\n    const chunks = [];\n    let start = 0;\n    \n    while (start < text.length) {\n      const end = Math.min(start + chunkSize, text.length);\n      const chunk = text.substring(start, end);\n      \n      chunks.push({\n        id: `chunk_${Date.now()}_${start}`,\n        text: chunk,\n        start: start,\n        end: end\n      });\n      \n      start = end - overlap;\n    }\n    \n    return chunks;\n  }\n\n  // Simple text similarity (cosine similarity)\n  calculateSimilarity(text1, text2) {\n    const words1 = text1.toLowerCase().split(/\\s+/);\n    const words2 = text2.toLowerCase().split(/\\s+/);\n    \n    const wordSet = new Set([...words1, ...words2]);\n    const vector1 = Array.from(wordSet).map(word => words1.filter(w => w === word).length);\n    const vector2 = Array.from(wordSet).map(word => words2.filter(w => w === word).length);\n    \n    const dotProduct = vector1.reduce((sum, val, i) => sum + val * vector2[i], 0);\n    const magnitude1 = Math.sqrt(vector1.reduce((sum, val) => sum + val * val, 0));\n    const magnitude2 = Math.sqrt(vector2.reduce((sum, val) => sum + val * val, 0));\n    \n    return dotProduct / (magnitude1 * magnitude2);\n  }\n\n  // Upload and process document\n  async uploadDocument(file) {\n    try {\n      // Extract text from file\n      const text = await this.extractTextFromFile(file);\n      \n      // Create document record\n      const document = {\n        id: `doc_${Date.now()}`,\n        filename: file.name,\n        fileType: file.type,\n        fileSize: file.size,\n        uploadDate: new Date().toISOString(),\n        text: text,\n        chunks: []\n      };\n\n      // Chunk the text\n      const chunks = this.chunkText(text);\n      document.chunks = chunks.map(chunk => chunk.id);\n      \n      // Add chunks to global chunks array\n      this.chunks.push(...chunks);\n      \n      // Add document to documents array\n      this.documents.push(document);\n      \n      // Save to localStorage\n      this.saveData();\n      \n      return {\n        success: true,\n        document: document,\n        chunksAdded: chunks.length,\n        totalChunks: this.chunks.length\n      };\n    } catch (error) {\n      console.error('Error uploading document:', error);\n      throw error;\n    }\n  }\n\n  // Search documents for relevant content\n  searchDocuments(query, maxResults = 5) {\n    const queryLower = query.toLowerCase();\n    const results = [];\n    \n    // Search through all chunks\n    this.chunks.forEach(chunk => {\n      const similarity = this.calculateSimilarity(queryLower, chunk.text.toLowerCase());\n      \n      if (similarity > 0.1) { // Threshold for relevance\n        results.push({\n          chunk: chunk,\n          similarity: similarity,\n          document: this.documents.find(doc => doc.chunks.includes(chunk.id))\n        });\n      }\n    });\n    \n    // Sort by similarity and return top results\n    return results\n      .sort((a, b) => b.similarity - a.similarity)\n      .slice(0, maxResults);\n  }\n\n  // Get all documents\n  getDocuments() {\n    return this.documents.map(doc => ({\n      id: doc.id,\n      filename: doc.filename,\n      fileType: doc.fileType,\n      fileSize: doc.fileSize,\n      uploadDate: doc.uploadDate,\n      chunkCount: doc.chunks.length\n    }));\n  }\n\n  // Delete document\n  deleteDocument(documentId) {\n    const documentIndex = this.documents.findIndex(doc => doc.id === documentId);\n    if (documentIndex === -1) {\n      throw new Error('Document not found');\n    }\n    \n    const document = this.documents[documentIndex];\n    \n    // Remove chunks associated with this document\n    this.chunks = this.chunks.filter(chunk => !document.chunks.includes(chunk.id));\n    \n    // Remove document\n    this.documents.splice(documentIndex, 1);\n    \n    // Save to localStorage\n    this.saveData();\n    \n    return { success: true };\n  }\n\n  // Crawl website content (simplified)\n  async crawlWebsite(url) {\n    try {\n      // This is a simplified web crawler\n      // In production, you'd use a proper web scraping library\n      const response = await fetch(`https://api.allorigins.win/get?url=${encodeURIComponent(url)}`);\n      const data = await response.json();\n      \n      if (data.contents) {\n        // Extract text content from HTML\n        const text = this.extractTextFromHTML(data.contents);\n        \n        // Create document record\n        const document = {\n          id: `web_${Date.now()}`,\n          filename: `Web Content - ${url}`,\n          fileType: 'text/html',\n          fileSize: text.length,\n          uploadDate: new Date().toISOString(),\n          text: text,\n          url: url,\n          chunks: []\n        };\n\n        // Chunk the text\n        const chunks = this.chunkText(text);\n        document.chunks = chunks.map(chunk => chunk.id);\n        \n        // Add chunks to global chunks array\n        this.chunks.push(...chunks);\n        \n        // Add document to documents array\n        this.documents.push(document);\n        \n        // Save to localStorage\n        this.saveData();\n        \n        return {\n          success: true,\n          document: document,\n          chunksAdded: chunks.length,\n          totalChunks: this.chunks.length\n        };\n      } else {\n        throw new Error('Failed to fetch website content');\n      }\n    } catch (error) {\n      console.error('Error crawling website:', error);\n      throw error;\n    }\n  }\n\n  // Extract text from HTML\n  extractTextFromHTML(html) {\n    // Simple HTML to text conversion\n    const div = document.createElement('div');\n    div.innerHTML = html;\n    \n    // Remove script and style elements\n    const scripts = div.querySelectorAll('script, style');\n    scripts.forEach(script => script.remove());\n    \n    // Get text content\n    let text = div.textContent || div.innerText || '';\n    \n    // Clean up text\n    text = text.replace(/\\s+/g, ' ').trim();\n    \n    return text.substring(0, 50000); // Limit text length\n  }\n\n  // Get document content for AI processing\n  getDocumentContent(documentId) {\n    const document = this.documents.find(doc => doc.id === documentId);\n    if (!document) {\n      throw new Error('Document not found');\n    }\n    \n    return document.text;\n  }\n\n  // Get relevant chunks for a query\n  getRelevantChunks(query, maxChunks = 10) {\n    const searchResults = this.searchDocuments(query, maxChunks);\n    return searchResults.map(result => ({\n      text: result.chunk.text,\n      similarity: result.similarity,\n      source: result.document.filename\n    }));\n  }\n}\n\nexport default DocumentManager; "],"mappings":"AAAA;AACA;;AAEA,MAAMA,eAAe,CAAC;EACpBC,WAAWA,CAAA,EAAG;IACZ,IAAI,CAACC,SAAS,GAAGC,IAAI,CAACC,KAAK,CAACC,YAAY,CAACC,OAAO,CAAC,iBAAiB,CAAC,IAAI,IAAI,CAAC;IAC5E,IAAI,CAACC,MAAM,GAAGJ,IAAI,CAACC,KAAK,CAACC,YAAY,CAACC,OAAO,CAAC,gBAAgB,CAAC,IAAI,IAAI,CAAC;IACxE,IAAI,CAACE,UAAU,GAAGL,IAAI,CAACC,KAAK,CAACC,YAAY,CAACC,OAAO,CAAC,oBAAoB,CAAC,IAAI,IAAI,CAAC;EAClF;;EAEA;EACAG,QAAQA,CAAA,EAAG;IACTJ,YAAY,CAACK,OAAO,CAAC,iBAAiB,EAAEP,IAAI,CAACQ,SAAS,CAAC,IAAI,CAACT,SAAS,CAAC,CAAC;IACvEG,YAAY,CAACK,OAAO,CAAC,gBAAgB,EAAEP,IAAI,CAACQ,SAAS,CAAC,IAAI,CAACJ,MAAM,CAAC,CAAC;IACnEF,YAAY,CAACK,OAAO,CAAC,oBAAoB,EAAEP,IAAI,CAACQ,SAAS,CAAC,IAAI,CAACH,UAAU,CAAC,CAAC;EAC7E;;EAEA;EACA,MAAMI,mBAAmBA,CAACC,IAAI,EAAE;IAC9B,OAAO,IAAIC,OAAO,CAAC,CAACC,OAAO,EAAEC,MAAM,KAAK;MACtC,MAAMC,MAAM,GAAG,IAAIC,UAAU,CAAC,CAAC;MAE/BD,MAAM,CAACE,MAAM,GAAG,MAAOC,CAAC,IAAK;QAC3B,IAAI;UACF,MAAMC,OAAO,GAAGD,CAAC,CAACE,MAAM,CAACC,MAAM;UAC/B,IAAIC,IAAI,GAAG,EAAE;UAEb,IAAIX,IAAI,CAACY,IAAI,KAAK,YAAY,EAAE;YAC9BD,IAAI,GAAGH,OAAO;UAChB,CAAC,MAAM,IAAIR,IAAI,CAACY,IAAI,KAAK,iBAAiB,EAAE;YAC1C;YACA;YACAD,IAAI,GAAG,IAAI,CAACE,kBAAkB,CAACL,OAAO,CAAC;UACzC,CAAC,MAAM,IAAIR,IAAI,CAACY,IAAI,CAACE,QAAQ,CAAC,MAAM,CAAC,IAAId,IAAI,CAACY,IAAI,CAACE,QAAQ,CAAC,UAAU,CAAC,EAAE;YACvE;YACAH,IAAI,GAAG,IAAI,CAACI,mBAAmB,CAACP,OAAO,CAAC;UAC1C,CAAC,MAAM;YACL;YACAG,IAAI,GAAGH,OAAO,CAACQ,QAAQ,CAAC,CAAC;UAC3B;UAEAd,OAAO,CAACS,IAAI,CAAC;QACf,CAAC,CAAC,OAAOM,KAAK,EAAE;UACdd,MAAM,CAACc,KAAK,CAAC;QACf;MACF,CAAC;MAEDb,MAAM,CAACc,OAAO,GAAGf,MAAM;MAEvB,IAAIH,IAAI,CAACY,IAAI,KAAK,YAAY,EAAE;QAC9BR,MAAM,CAACe,UAAU,CAACnB,IAAI,CAAC;MACzB,CAAC,MAAM;QACLI,MAAM,CAACgB,iBAAiB,CAACpB,IAAI,CAAC;MAChC;IACF,CAAC,CAAC;EACJ;;EAEA;EACAa,kBAAkBA,CAACQ,WAAW,EAAE;IAC9B;IACA,MAAMC,UAAU,GAAG,IAAIC,UAAU,CAACF,WAAW,CAAC;IAC9C,MAAMV,IAAI,GAAG,IAAIa,WAAW,CAAC,CAAC,CAACC,MAAM,CAACH,UAAU,CAAC;;IAEjD;IACA,MAAMI,SAAS,GAAGf,IAAI,CAACgB,KAAK,CAAC,cAAc,CAAC;IAC5C,IAAID,SAAS,EAAE;MACb,OAAOA,SAAS,CAACE,IAAI,CAAC,GAAG,CAAC,CAACC,OAAO,CAAC,UAAU,EAAE,GAAG,CAAC;IACrD;IAEA,OAAOlB,IAAI,CAACkB,OAAO,CAAC,UAAU,EAAE,GAAG,CAAC,CAACC,SAAS,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,CAAC;EAC5D;;EAEA;EACAf,mBAAmBA,CAACM,WAAW,EAAE;IAC/B,MAAMC,UAAU,GAAG,IAAIC,UAAU,CAACF,WAAW,CAAC;IAC9C,MAAMV,IAAI,GAAG,IAAIa,WAAW,CAAC,CAAC,CAACC,MAAM,CAACH,UAAU,CAAC;;IAEjD;IACA,MAAMI,SAAS,GAAGf,IAAI,CAACgB,KAAK,CAAC,2BAA2B,CAAC;IACzD,IAAID,SAAS,EAAE;MACb,OAAOA,SAAS,CAACK,GAAG,CAACJ,KAAK,IAAIA,KAAK,CAACE,OAAO,CAAC,UAAU,EAAE,EAAE,CAAC,CAAC,CAACD,IAAI,CAAC,GAAG,CAAC;IACxE;IAEA,OAAOjB,IAAI,CAACkB,OAAO,CAAC,UAAU,EAAE,GAAG,CAAC,CAACC,SAAS,CAAC,CAAC,EAAE,KAAK,CAAC;EAC1D;;EAEA;EACAE,SAASA,CAACrB,IAAI,EAAEsB,SAAS,GAAG,IAAI,EAAEC,OAAO,GAAG,GAAG,EAAE;IAC/C,MAAMxC,MAAM,GAAG,EAAE;IACjB,IAAIyC,KAAK,GAAG,CAAC;IAEb,OAAOA,KAAK,GAAGxB,IAAI,CAACyB,MAAM,EAAE;MAC1B,MAAMC,GAAG,GAAGC,IAAI,CAACC,GAAG,CAACJ,KAAK,GAAGF,SAAS,EAAEtB,IAAI,CAACyB,MAAM,CAAC;MACpD,MAAMI,KAAK,GAAG7B,IAAI,CAACmB,SAAS,CAACK,KAAK,EAAEE,GAAG,CAAC;MAExC3C,MAAM,CAAC+C,IAAI,CAAC;QACVC,EAAE,EAAE,SAASC,IAAI,CAACC,GAAG,CAAC,CAAC,IAAIT,KAAK,EAAE;QAClCxB,IAAI,EAAE6B,KAAK;QACXL,KAAK,EAAEA,KAAK;QACZE,GAAG,EAAEA;MACP,CAAC,CAAC;MAEFF,KAAK,GAAGE,GAAG,GAAGH,OAAO;IACvB;IAEA,OAAOxC,MAAM;EACf;;EAEA;EACAmD,mBAAmBA,CAACC,KAAK,EAAEC,KAAK,EAAE;IAChC,MAAMC,MAAM,GAAGF,KAAK,CAACG,WAAW,CAAC,CAAC,CAACC,KAAK,CAAC,KAAK,CAAC;IAC/C,MAAMC,MAAM,GAAGJ,KAAK,CAACE,WAAW,CAAC,CAAC,CAACC,KAAK,CAAC,KAAK,CAAC;IAE/C,MAAME,OAAO,GAAG,IAAIC,GAAG,CAAC,CAAC,GAAGL,MAAM,EAAE,GAAGG,MAAM,CAAC,CAAC;IAC/C,MAAMG,OAAO,GAAGC,KAAK,CAACC,IAAI,CAACJ,OAAO,CAAC,CAACrB,GAAG,CAAC0B,IAAI,IAAIT,MAAM,CAACU,MAAM,CAACC,CAAC,IAAIA,CAAC,KAAKF,IAAI,CAAC,CAACrB,MAAM,CAAC;IACtF,MAAMwB,OAAO,GAAGL,KAAK,CAACC,IAAI,CAACJ,OAAO,CAAC,CAACrB,GAAG,CAAC0B,IAAI,IAAIN,MAAM,CAACO,MAAM,CAACC,CAAC,IAAIA,CAAC,KAAKF,IAAI,CAAC,CAACrB,MAAM,CAAC;IAEtF,MAAMyB,UAAU,GAAGP,OAAO,CAACQ,MAAM,CAAC,CAACC,GAAG,EAAEC,GAAG,EAAEC,CAAC,KAAKF,GAAG,GAAGC,GAAG,GAAGJ,OAAO,CAACK,CAAC,CAAC,EAAE,CAAC,CAAC;IAC7E,MAAMC,UAAU,GAAG5B,IAAI,CAAC6B,IAAI,CAACb,OAAO,CAACQ,MAAM,CAAC,CAACC,GAAG,EAAEC,GAAG,KAAKD,GAAG,GAAGC,GAAG,GAAGA,GAAG,EAAE,CAAC,CAAC,CAAC;IAC9E,MAAMI,UAAU,GAAG9B,IAAI,CAAC6B,IAAI,CAACP,OAAO,CAACE,MAAM,CAAC,CAACC,GAAG,EAAEC,GAAG,KAAKD,GAAG,GAAGC,GAAG,GAAGA,GAAG,EAAE,CAAC,CAAC,CAAC;IAE9E,OAAOH,UAAU,IAAIK,UAAU,GAAGE,UAAU,CAAC;EAC/C;;EAEA;EACA,MAAMC,cAAcA,CAACrE,IAAI,EAAE;IACzB,IAAI;MACF;MACA,MAAMW,IAAI,GAAG,MAAM,IAAI,CAACZ,mBAAmB,CAACC,IAAI,CAAC;;MAEjD;MACA,MAAMsE,QAAQ,GAAG;QACf5B,EAAE,EAAE,OAAOC,IAAI,CAACC,GAAG,CAAC,CAAC,EAAE;QACvB2B,QAAQ,EAAEvE,IAAI,CAACwE,IAAI;QACnBC,QAAQ,EAAEzE,IAAI,CAACY,IAAI;QACnB8D,QAAQ,EAAE1E,IAAI,CAAC2E,IAAI;QACnBC,UAAU,EAAE,IAAIjC,IAAI,CAAC,CAAC,CAACkC,WAAW,CAAC,CAAC;QACpClE,IAAI,EAAEA,IAAI;QACVjB,MAAM,EAAE;MACV,CAAC;;MAED;MACA,MAAMA,MAAM,GAAG,IAAI,CAACsC,SAAS,CAACrB,IAAI,CAAC;MACnC2D,QAAQ,CAAC5E,MAAM,GAAGA,MAAM,CAACqC,GAAG,CAACS,KAAK,IAAIA,KAAK,CAACE,EAAE,CAAC;;MAE/C;MACA,IAAI,CAAChD,MAAM,CAAC+C,IAAI,CAAC,GAAG/C,MAAM,CAAC;;MAE3B;MACA,IAAI,CAACL,SAAS,CAACoD,IAAI,CAAC6B,QAAQ,CAAC;;MAE7B;MACA,IAAI,CAAC1E,QAAQ,CAAC,CAAC;MAEf,OAAO;QACLkF,OAAO,EAAE,IAAI;QACbR,QAAQ,EAAEA,QAAQ;QAClBS,WAAW,EAAErF,MAAM,CAAC0C,MAAM;QAC1B4C,WAAW,EAAE,IAAI,CAACtF,MAAM,CAAC0C;MAC3B,CAAC;IACH,CAAC,CAAC,OAAOnB,KAAK,EAAE;MACdgE,OAAO,CAAChE,KAAK,CAAC,2BAA2B,EAAEA,KAAK,CAAC;MACjD,MAAMA,KAAK;IACb;EACF;;EAEA;EACAiE,eAAeA,CAACC,KAAK,EAAEC,UAAU,GAAG,CAAC,EAAE;IACrC,MAAMC,UAAU,GAAGF,KAAK,CAAClC,WAAW,CAAC,CAAC;IACtC,MAAMqC,OAAO,GAAG,EAAE;;IAElB;IACA,IAAI,CAAC5F,MAAM,CAAC6F,OAAO,CAAC/C,KAAK,IAAI;MAC3B,MAAMgD,UAAU,GAAG,IAAI,CAAC3C,mBAAmB,CAACwC,UAAU,EAAE7C,KAAK,CAAC7B,IAAI,CAACsC,WAAW,CAAC,CAAC,CAAC;MAEjF,IAAIuC,UAAU,GAAG,GAAG,EAAE;QAAE;QACtBF,OAAO,CAAC7C,IAAI,CAAC;UACXD,KAAK,EAAEA,KAAK;UACZgD,UAAU,EAAEA,UAAU;UACtBlB,QAAQ,EAAE,IAAI,CAACjF,SAAS,CAACoG,IAAI,CAACC,GAAG,IAAIA,GAAG,CAAChG,MAAM,CAACoB,QAAQ,CAAC0B,KAAK,CAACE,EAAE,CAAC;QACpE,CAAC,CAAC;MACJ;IACF,CAAC,CAAC;;IAEF;IACA,OAAO4C,OAAO,CACXK,IAAI,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKA,CAAC,CAACL,UAAU,GAAGI,CAAC,CAACJ,UAAU,CAAC,CAC3CM,KAAK,CAAC,CAAC,EAAEV,UAAU,CAAC;EACzB;;EAEA;EACAW,YAAYA,CAAA,EAAG;IACb,OAAO,IAAI,CAAC1G,SAAS,CAAC0C,GAAG,CAAC2D,GAAG,KAAK;MAChChD,EAAE,EAAEgD,GAAG,CAAChD,EAAE;MACV6B,QAAQ,EAAEmB,GAAG,CAACnB,QAAQ;MACtBE,QAAQ,EAAEiB,GAAG,CAACjB,QAAQ;MACtBC,QAAQ,EAAEgB,GAAG,CAAChB,QAAQ;MACtBE,UAAU,EAAEc,GAAG,CAACd,UAAU;MAC1BoB,UAAU,EAAEN,GAAG,CAAChG,MAAM,CAAC0C;IACzB,CAAC,CAAC,CAAC;EACL;;EAEA;EACA6D,cAAcA,CAACC,UAAU,EAAE;IACzB,MAAMC,aAAa,GAAG,IAAI,CAAC9G,SAAS,CAAC+G,SAAS,CAACV,GAAG,IAAIA,GAAG,CAAChD,EAAE,KAAKwD,UAAU,CAAC;IAC5E,IAAIC,aAAa,KAAK,CAAC,CAAC,EAAE;MACxB,MAAM,IAAIE,KAAK,CAAC,oBAAoB,CAAC;IACvC;IAEA,MAAM/B,QAAQ,GAAG,IAAI,CAACjF,SAAS,CAAC8G,aAAa,CAAC;;IAE9C;IACA,IAAI,CAACzG,MAAM,GAAG,IAAI,CAACA,MAAM,CAACgE,MAAM,CAAClB,KAAK,IAAI,CAAC8B,QAAQ,CAAC5E,MAAM,CAACoB,QAAQ,CAAC0B,KAAK,CAACE,EAAE,CAAC,CAAC;;IAE9E;IACA,IAAI,CAACrD,SAAS,CAACiH,MAAM,CAACH,aAAa,EAAE,CAAC,CAAC;;IAEvC;IACA,IAAI,CAACvG,QAAQ,CAAC,CAAC;IAEf,OAAO;MAAEkF,OAAO,EAAE;IAAK,CAAC;EAC1B;;EAEA;EACA,MAAMyB,YAAYA,CAACC,GAAG,EAAE;IACtB,IAAI;MACF;MACA;MACA,MAAMC,QAAQ,GAAG,MAAMC,KAAK,CAAC,sCAAsCC,kBAAkB,CAACH,GAAG,CAAC,EAAE,CAAC;MAC7F,MAAMI,IAAI,GAAG,MAAMH,QAAQ,CAACI,IAAI,CAAC,CAAC;MAElC,IAAID,IAAI,CAACE,QAAQ,EAAE;QACjB;QACA,MAAMnG,IAAI,GAAG,IAAI,CAACoG,mBAAmB,CAACH,IAAI,CAACE,QAAQ,CAAC;;QAEpD;QACA,MAAMxC,QAAQ,GAAG;UACf5B,EAAE,EAAE,OAAOC,IAAI,CAACC,GAAG,CAAC,CAAC,EAAE;UACvB2B,QAAQ,EAAE,iBAAiBiC,GAAG,EAAE;UAChC/B,QAAQ,EAAE,WAAW;UACrBC,QAAQ,EAAE/D,IAAI,CAACyB,MAAM;UACrBwC,UAAU,EAAE,IAAIjC,IAAI,CAAC,CAAC,CAACkC,WAAW,CAAC,CAAC;UACpClE,IAAI,EAAEA,IAAI;UACV6F,GAAG,EAAEA,GAAG;UACR9G,MAAM,EAAE;QACV,CAAC;;QAED;QACA,MAAMA,MAAM,GAAG,IAAI,CAACsC,SAAS,CAACrB,IAAI,CAAC;QACnC2D,QAAQ,CAAC5E,MAAM,GAAGA,MAAM,CAACqC,GAAG,CAACS,KAAK,IAAIA,KAAK,CAACE,EAAE,CAAC;;QAE/C;QACA,IAAI,CAAChD,MAAM,CAAC+C,IAAI,CAAC,GAAG/C,MAAM,CAAC;;QAE3B;QACA,IAAI,CAACL,SAAS,CAACoD,IAAI,CAAC6B,QAAQ,CAAC;;QAE7B;QACA,IAAI,CAAC1E,QAAQ,CAAC,CAAC;QAEf,OAAO;UACLkF,OAAO,EAAE,IAAI;UACbR,QAAQ,EAAEA,QAAQ;UAClBS,WAAW,EAAErF,MAAM,CAAC0C,MAAM;UAC1B4C,WAAW,EAAE,IAAI,CAACtF,MAAM,CAAC0C;QAC3B,CAAC;MACH,CAAC,MAAM;QACL,MAAM,IAAIiE,KAAK,CAAC,iCAAiC,CAAC;MACpD;IACF,CAAC,CAAC,OAAOpF,KAAK,EAAE;MACdgE,OAAO,CAAChE,KAAK,CAAC,yBAAyB,EAAEA,KAAK,CAAC;MAC/C,MAAMA,KAAK;IACb;EACF;;EAEA;EACA8F,mBAAmBA,CAACC,IAAI,EAAE;IACxB;IACA,MAAMC,GAAG,GAAG3C,QAAQ,CAAC4C,aAAa,CAAC,KAAK,CAAC;IACzCD,GAAG,CAACE,SAAS,GAAGH,IAAI;;IAEpB;IACA,MAAMI,OAAO,GAAGH,GAAG,CAACI,gBAAgB,CAAC,eAAe,CAAC;IACrDD,OAAO,CAAC7B,OAAO,CAAC+B,MAAM,IAAIA,MAAM,CAACC,MAAM,CAAC,CAAC,CAAC;;IAE1C;IACA,IAAI5G,IAAI,GAAGsG,GAAG,CAACO,WAAW,IAAIP,GAAG,CAACQ,SAAS,IAAI,EAAE;;IAEjD;IACA9G,IAAI,GAAGA,IAAI,CAACkB,OAAO,CAAC,MAAM,EAAE,GAAG,CAAC,CAAC6F,IAAI,CAAC,CAAC;IAEvC,OAAO/G,IAAI,CAACmB,SAAS,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,CAAC;EACnC;;EAEA;EACA6F,kBAAkBA,CAACzB,UAAU,EAAE;IAC7B,MAAM5B,QAAQ,GAAG,IAAI,CAACjF,SAAS,CAACoG,IAAI,CAACC,GAAG,IAAIA,GAAG,CAAChD,EAAE,KAAKwD,UAAU,CAAC;IAClE,IAAI,CAAC5B,QAAQ,EAAE;MACb,MAAM,IAAI+B,KAAK,CAAC,oBAAoB,CAAC;IACvC;IAEA,OAAO/B,QAAQ,CAAC3D,IAAI;EACtB;;EAEA;EACAiH,iBAAiBA,CAACzC,KAAK,EAAE0C,SAAS,GAAG,EAAE,EAAE;IACvC,MAAMC,aAAa,GAAG,IAAI,CAAC5C,eAAe,CAACC,KAAK,EAAE0C,SAAS,CAAC;IAC5D,OAAOC,aAAa,CAAC/F,GAAG,CAACrB,MAAM,KAAK;MAClCC,IAAI,EAAED,MAAM,CAAC8B,KAAK,CAAC7B,IAAI;MACvB6E,UAAU,EAAE9E,MAAM,CAAC8E,UAAU;MAC7BuC,MAAM,EAAErH,MAAM,CAAC4D,QAAQ,CAACC;IAC1B,CAAC,CAAC,CAAC;EACL;AACF;AAEA,eAAepF,eAAe","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}